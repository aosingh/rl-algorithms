{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stochastic Dynamic Programs\n",
    "\n",
    "*Notes created using the Textbook **\"Approximate Dynamic Programming\" by Dr. Warren B. Powell***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic dynamic programs, at minimum, consist of the following elements\n",
    "\n",
    "1. **State variable** : This captures all the information we need to make a decision, as well as the information that we need to describe how the system evolves over time\n",
    "\n",
    "2. **Decision variable** : Decisions/Actions represent how we control the process.\n",
    "\n",
    "3. **Exogenous information** : This is the data that first become known each time period (e.g. the demand for product, or the price at which it can be purchased or sold)\n",
    "\n",
    "4. **Transition function** : The function determines how the system evolves from the state $S_t$ to the state $S_{t+1}$ given the decision that was made at time $t$ and the new information that arrived between $t$ and $t+1$.\n",
    "\n",
    "5. **Objection function** : The function specifies the costs being minimized or the contributions/rewards being maximized, over a time horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Asset Acquisition Problem : Here asset that we are acquiring could be stocks, planes, oil, etc. Moslty, in these problems, we need to satisfy random demands.\n",
    "\n",
    "* State Variable $S_t$\n",
    "    - $R_t$ = Asset in hand before making any decision.\n",
    "    - $D_t$ = Actual demand at time t.\n",
    "    - $p_t$ = Price time at time t.\n",
    " \n",
    " Thus $S_t$ = $(R_t, D_t, p_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actions - We need to make 2 decisions\n",
    " - $x^D_{t}$ = How much of the demand $D_t$, during the time interval t, can we satisfy using the available assets $R_t$. Clearly, $x^D_{t} <= R_t$\n",
    " - $x^O_{t}$ = How much new asset should be acquired at time $t$, which can be used to satisfy demands at time $t+1$ \n",
    " \n",
    " **Note**: We can also have continous of vector valued decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exogenous information consists of 3 types of information\n",
    "    - $\\hat{D}_t$ = New demands that arise during the time interval t\n",
    "    - $\\hat{p}_t$ = Change in the price at which we sell our assets\n",
    "    - $\\hat{R}_t$ = Finally, some exogenous changes to our available resources\n",
    "\n",
    "We use a generic variable $W_t$ to represent all the new information that is first learned during the time interval $t$. Thus $$W_t = (\\hat{R}_t, \\hat{D}_t, \\hat{p}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to specifying the types of exogenous information, for stochastic models we also have to specify the likelihood of a particular outcome. This might come in the form of an assumed probability distribution for $\\hat{R}_t, \\hat{D}_t \\text{and } \\hat{p}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have determined an action we are going to take from our decision rule, we compute our contribution $C_t(S_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our asset acquisition problem, the contribution function is: $$C_t(S_t, a_t) = p_ta^D_t - c_ta^O_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is; Total Selling price - Total purchase price. In this particular model $C_t(S_t, a_t)$ is a deterministic function of state and action.\n",
    "\n",
    "**In other applications, the contribution from action $a_t$ depends on what happends during time $t+1$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to specify how the state variable changes over time. This is done using the transition function. $$S_{t+1} = S^M(S_t, a_t, W_t+1)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superscript $M$ stands for \"model\" or \"system model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in our asset acquisition problem the transition function is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R_{t+1} = R_{t} - a^D_t + a^O_t + \\hat{R}_{t+1}$$\n",
    "\n",
    "$$D_{t+1} = D_{t} - a^D_t + \\hat{D}_{t+1}$$\n",
    "\n",
    "$$p_{t+1} = p_t + \\hat{p}_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the objective function. Suppose, we are trying to maximize the total contribution received over a finite horizon t = (0, 1, ..... T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deterministic problem, the objective function is $$ max_{(a_t)^T_{t=0}} \\sum_{t=0}^{T} C_t(S_t, a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a stochastic problem, there are a number of possible realizations of the exogenous information $(W_t)_{t=0}^T$.\n",
    "\n",
    "The exogenous information process is uncertain, we do not know which state we will be in time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get around this problem by finding the best policy(decision rule) for choosing decisions. This policy must be chosen to produce the best **expected** contribution over all outcomes. Let $A^\\pi(S_t)$ be a particular rule indexed by $\\pi$ and let $\\prod$ be a set of decision rules, then the problem of finding the best policy would be written as \n",
    "\n",
    "$$ max_{\\pi \\in \\prod} \\mathbb{E} \\sum_{t=0}^T C_t(S_t, A^\\pi (S_t))\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the problem of finding the best policy is to maximize the expected value of total contribution over our horizon.\n",
    "There are many variations of this problem. For applications where the horizon is long enough to affect the time value of the money, we might introduce a discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ max_{\\pi \\in \\prod} \\mathbb{E} \\sum_{t=0}^T \\gamma^{t}C_t(S_t, A^\\pi (S_t))\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, A **complete** specification of a dynamic program requires that we specify both data and functions, as follows\n",
    "\n",
    "* Data\n",
    "    - Initial state $S_0$\n",
    "    - The exogenous information process $W_t$\n",
    "\n",
    "\n",
    "* Functions\n",
    "    - The Contribution function $C(S_t, a_t)$\n",
    "    - The Transition function $S_{t+1} = S^M(S_t, a_t, W_t+1)$\n",
    "    - The family of decision functions $(A^{\\pi}(S))_{\\pi \\in \\prod}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
