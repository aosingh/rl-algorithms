{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "*Notes created using the Textbook **\"Approximate Dynamic Programming\" by Dr. Warren B. Powell***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving Stochastic Dynamic Programs within some fairly limiting assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume that we have a discrete state space, where $S$ is small enough to enumerate $$S = (1, 2, ...., |S|)$$\n",
    "\n",
    "2. Assume that there is a relatively small set of decisions or actions, which we denote by \n",
    "\\begin{align}\n",
    "a \\text{ } \\epsilon \\text{ } A\n",
    "\\end{align}\n",
    "\n",
    "3. Finally, assume that we are given a transition matrix. This matrix gives the probability that if we in state $S_{t}$ and take action $a_{t}$ then we will next be in state $S_{t+1}$ \n",
    "$$p_{t}(S_{t+1} | S_{t}, a_{t})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimality equations\n",
    "\n",
    "$$ max_{\\pi \\in \\prod} \\mathbb{E} \\sum_{t=0}^T \\gamma^{t}C_t(S_t, A^\\pi (S_t))  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation is computationally intractable. However, we do not have to solve that equation at once. \n",
    "\n",
    "General idea : If we are in state $S_{t} = i$, our transition function will tell us the state we will land in if we take action $a_{t}$ using $S_{t+1} = S^{M}(S_{t}, a_{t})$\n",
    "\n",
    "**Question** : How do we know which action $a_t$ is the best action. \n",
    "\n",
    "**Answer** : Suppose we have a function $V_{t+1}(S_{t+1})$ that told us about the value of being in state $S_{t+1}$. \n",
    "Then, we could evaluate each possible action $a_t$ and choose the action that has the largest one period Contribution\n",
    "$C_t(S_t, a_t)$ plus the value of landing in state $S_{t+1} = S^M(S_t, a_t)$, which we represent using $V_{t+1}(S_{t+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to solve \n",
    "$$a^*_{t}(S_{t}) = arg max_{a_t \\epsilon A_t}(C_t(S_t, a_t) + \\gamma V_{t+1}(S_{t+1}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ is called the discount factor because we depend on the value function which is one time period in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmax$ means we want to choose the action $a_t$ that maximises the expression in parenthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $S_{t+1}$ is a function of $S_t$ and $a_t$, meaning it can be written as $S_{t+1}(S_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of being in state $S_t$ is the value of using the optimal decision $a^*_{t}(S_{t})$. That is\n",
    "\n",
    "$$V_{t}(S_t) =  max_{a_t \\epsilon A_t}(C_t(S_t, a_t) + \\gamma V_{t+1}(S_{t+1}(S_t, a_t)))$$\n",
    "$$= C_t(S_t, a^*_{t}(S_{t})) + \\gamma V_{t+1}(S_{t+1}(S_t, a^*_{t}(S_{t}))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation is the optimality equation for **deterministic problems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **STOCHASTIC** problems, we have to model the fact that new information becomes available only after we make the decision $a_t$. Thus we have uncertainity both in contribution earned and determination of the next state we visit, $S_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the determinisitic optimality equations by simply adding an expectation. This is known as the standard form of Bellman's equations\n",
    "$$ V_{t}(S_t) = max_{a_t \\epsilon A_t}(C_t(S_t, a_t) + \\gamma \\sum_{s^{'} \\epsilon S} P(S_{t+1} = s^{'}|S_t, a_t)V_{t+1}(s^{'}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent more natural form for approximate dynamic programming is to write\n",
    "$$ V_{t}(S_t) = max_{a_t \\epsilon A_t}(C_t(S_t, a_t) + \\gamma \\mathop{\\mathbb{E}\\{V_{t+1}(S_{t+1})| S_{t}\\}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that $S_{t+1}$ is functionally dependent on $S_t$ and $a_t$ or probabilistically dependent. If the expectation is only functionally dependent then, the above equation can be written as \n",
    "$$ V_{t}(S_t) = max_{a_t \\epsilon A_t}(C_t(S_t, a_t) + \\gamma \\mathop{\\mathbb{E}\\{V_{t+1}(S_{t+1})\\}})$$\n",
    "Here the exepection $\\mathop{\\mathbb{E}}$ is not conditioned on anything, but there is functional dependence of $S_{t+1}$ on $S_t$ and $a_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://aima.cs.berkeley.edu/python/mdp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a policy $\\pi$ is a rule that specifies the action $a_t$ given the state $S_t$. In simplest form, it is a lookup table. But in real problems, the representation of policy is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that we transition from state $S_t = s$ to $S_{t+1} = s^{'} $ can be written as\n",
    "$$p_{ss^{'}}(a) = P(S_{t+1} = s^{'} | S_t = s, a_t = a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a function $A_t^{\\pi}(s)$ that determines the action a we should take when in state s. It is common to write the transition probability $p_{ss^{'}}(a)$ in the form\n",
    "\n",
    "$$p_{ss^{'}}^{\\pi} = P(S_{t+1} = s^{'} | S_t = s, A_t^{\\pi}(s) = a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write this in the matrix form\n",
    "\n",
    "$$P_{t}^{\\pi} = $$ one-step transtion matrix under the policy $\\pi$, where $p_{ss^{'}}^{\\pi}$ is an element in the row s and column $s^{'}$. There is a different matrix $P^{\\pi}$ for each policy or decision rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let $c_{t}^{\\pi}$ be a column vector with element $c_t^{\\pi}(s) = C_t(s, A_t^{\\pi}(s))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let $v_{t+1}$ be a column vector with element $V_{t+1}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matrix/vector form, we can write\n",
    "$$v_t = max_{\\pi} (c_t^{\\pi} + \\gamma P_t^{\\pi}v_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we maximise over policies because we want to find the best action for each state. The vector $v_t$ is called the **value function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation can be solved by finding $a_t$ for each state $s$. The result is a decision vector $a_{t}^{*} = (a_{t}^{*}(S))_{s \\epsilon S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus determining the decision vector is equivalent of determining the best policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the expected profit using a policy $\\pi$ from time t onward:\n",
    "$$ F_{t}^{\\pi}(S_t) = \\mathop{\\mathbb{E}} \\{ \\sum_{t^{'}=0}^T C_{t^{'}}(S_{t^{'}}, A_{t^{'}}^{\\pi}(S_{t^{'}})) + C_{T}(S_T)| S_t \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected contribution if we are in state $S_t$ in time t and follow policy $\\pi$ from time t onward. This is not that easy to calculate and it seems more natural to calculate $V_{t}^{\\pi}$ recursively.\n",
    "$$V_t^{\\pi}(S_t) = C_t(S_t, A_t^{\\pi}(S_t)) + \\mathop{\\mathbb{E}} \\{ V_{t+1}^{\\pi} (S_{t+1})\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important \n",
    "\n",
    "$$V_t^{\\pi}(S_t) = F_t^{\\pi}(S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common in Stochastic Dynamic Programming(more precisely, Markov Decision Process) to assume that the one-step transition matrix $P^{\\pi}$ is given as data. \n",
    "\n",
    "Remember, there is a different matrix for each policy $\\pi$.\n",
    "\n",
    "In practice, we assume that we know the transition function $S^{M}(S_t, a_t, W_{t+1})$ from which we can derive the one-step transition matrix\n",
    "\n",
    "Assume that the random information that arrives between time $t$ and $t+1$ is independent of all the prior information.\n",
    "\n",
    "Let $\\Omega _ {t+1}$ be the set of possible outcomes of $W_{t+1}$ where $\\mathop{\\mathbb{P}} (W_{t+1} = \\omega _{t+1})$ is the probability of outcome $\\omega _{t+1} \\epsilon \\Omega _{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define the indicator function\n",
    "$$\n",
    "1_{\\{X\\}} = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\text {If the statement \"X\" is true.}\\\\\n",
    "            0 & \\text {otherwise.}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one step transition probability can be written as :\n",
    "$$\n",
    "\\mathop{\\mathbb{P}(S_{t+1}| S_t, a_t) = \\mathop{\\mathbb{E}}1_{s^{'} = S^{M}(S_t, a_t, W_{t+1})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{\\omega _{t+1} \\epsilon \\Omega _{t+1}} \\mathop{\\mathbb{P}}(\\omega _{t+1}) 1_{s^{'} = S^{M}(S_t, a_t, W_{t+1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to find the one-step transition matrix, all we have to do is to sum over all possible outcomes of the information $W_{t+1}$ and add up probabilities that take us from a particular state-action pair $(S_t, a_t)$ to a particular state $S_{t+1} = s^{'}$ \n",
    "\n",
    "This is not easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random contributions\n",
    "\n",
    "In many applications, the one-period contribution function is a deterministic function of $S_t$ and $a_t$, and hence we routinely write the contribution as the deterministic function $C_t(S_t, a_t)$. \n",
    "\n",
    "However, this is not always the case.\n",
    "\n",
    "Let us define a contribution function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{C} _{t+1}(S_t, a_t, W _{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= Contribution received in period $t+1$ given the state $S_t$ and decision $a_t$, as well as the new information $W_{t+1}$ that arrives in period $t+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we simply bring the expectation in the front giving us \n",
    "$$V_t(S_t) = max_{a_t} \\mathop{\\mathbb{E}} \\{ \\hat{C} _{t+1}(S_t, a_t, W _{t+1}) + \\gamma V_{t+1}(S_{t+1}) | S_t\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let\n",
    "\n",
    "$$C_t(S_t, a_t) = \\mathop{\\mathbb{E}} \\{ \\hat{C}_{t+1}(S_t, a_t, W_{t+1})| S_t\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Horizon problems\n",
    "\n",
    "For example, we might be interested in the value of an American option where we are allowed to sell an asset at any time $t <= T$, where $T$ is the exercise date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we encounter a finite horizon problem, we assume that we are given the function $V_T(S_T)$ as data. Often we simply use $V_T(S_T)$ = 0 because we are primarily interested in what to now, given by $a_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Dynamic Programming\n",
    "Solving a finite horizon problem is pretty straight forward. We simply start at the last time period, compute the value function for each possible state $s \\epsilon S$, and then step back another time period. \n",
    "\n",
    "That way at time t we have already computed $V_{t+1}(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 0**: Initialization\n",
    "1. Initialize the terminal contribution $V_{T}(S_T)$\n",
    "2. Set $t = T-1$\n",
    "\n",
    "\n",
    "**STEP 1** Calculate for all $S_t \\epsilon S$:\n",
    "\n",
    "$$V_t(S_t) = max_{a_{t}} \\{ C_t(S_t, a_t) + \\gamma \\sum_{s^{'} \\epsilon S} \\mathop{\\mathbb{P}}(s^{'}| S_t, a_t)V_{t+1}(s^{'})\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2** if t > 0 return to STEP 1. Else Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite Horizon Problems\n",
    "\n",
    "Often, we wish to study the problems in steady state. The infinite horizon problem is :\n",
    "\n",
    "$$ max_{\\pi \\epsilon \\Pi} \\mathop{\\mathbb{E}} \\{ \\sum_{t=0}^{\\infty} \\gamma ^{t} C_t(S_t, A_t^{\\pi}(S_t))\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define \n",
    "\n",
    "$$P^{\\pi, t} = \\text{t-step transition matrix over periods 0, 1, 2, ...., t-1 given policy} \\pi  $$\n",
    "$$ = \\Pi_{t^{'} = 0}^{t-1} P_{t^{'}}^\\pi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further define $P^{\\pi, 0}$ to be identity matrix. As before, let "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
